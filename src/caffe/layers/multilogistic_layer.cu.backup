#include <algorithm>
#include <cfloat>
#include <vector>

#include "thrust/device_vector.h"

#include "caffe/layer.hpp"
#include "caffe/util/math_functions.hpp"
#include "caffe/vision_layers.hpp"

namespace caffe {


template <typename Dtype>
__global__ void kernel_sub(const int count,const Dtype* data, Dtype* out) {
  CUDA_KERNEL_LOOP(index, count) {
    out[index] -= data[index];
  }
}

template <typename Dtype>
__global__ void kernel_add(const int count,const Dtype* data, Dtype* out) {
  CUDA_KERNEL_LOOP(index, count) {
    out[index] += data[index];
  }
}


template <typename Dtype>
__global__ void kernel_mul(const int count,const Dtype* data, Dtype* out) {
  CUDA_KERNEL_LOOP(index, count) {
    out[index] *= data[index];
  }
}

template <typename Dtype>
__global__ void kernel_div(const int count,const Dtype* data, Dtype* out) {
  CUDA_KERNEL_LOOP(index, count) {
    out[index] /= data[index];
  }
}
template <typename Dtype>
__global__ void kernel_rec(const int count,const Dtype* data, Dtype* out) {
  CUDA_KERNEL_LOOP(index, count) {
    out[index] = 1/data[index];
  }
}
template <typename Dtype>
__global__ void kernel_exp(const int count, const Dtype* data, Dtype* out) {
  CUDA_KERNEL_LOOP(index, count) {
    out[index] = exp(data[index]);
  }
}

template <typename Dtype>
void MultilogisticLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
	LOG(INFO) <<" Entered multilogistic layer !";
  const Dtype* bottom_data = bottom[0]->gpu_data();
  Dtype* top_data = top[0]->mutable_gpu_data();
  Dtype* scale_data = scale_.mutable_gpu_data();
  int count = bottom[0]->count();
  int channels = top[0]->shape(multilogistic_axis_);
  caffe_copy(count, bottom_data, top_data);
  // negation
  // NOLINT_NEXT_LINE(whitespace/operators)
	LOG(INFO) <<" Entered multilogistic layer 0 !";
  LOG(INFO)<< count;
  LOG(INFO)<< scale_.count();
  caffe_gpu_set(scale_.count(),Dtype(-1),scale_data);
  LOG(INFO)<< count;

  kernel_mul<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, scale_data, top_data);
  // exponentiate
  // NOLINT_NEXT_LINE(whitespace/operators)
	LOG(INFO) <<" Entered multilogistic layer 1 !";
  kernel_exp<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, top_data, top_data);
  // plus one
  // NOLINT_NEXT_LINE(whitespace/operators)
	LOG(INFO) <<" Entered multilogistic layer 2 !";
  caffe_gpu_set(scale_.count(),Dtype(1),scale_data);
  kernel_add<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, scale_data, top_data);
  // divide
  // NOLINT_NEXT_LINE(whitespace/operators)
	LOG(INFO) <<" Entered multilogistic layer 3 !";
  kernel_rec<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, top_data , top_data);
}

template <typename Dtype>
void MultilogisticLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,
    const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {
  const Dtype* top_diff = top[0]->gpu_diff();
  const Dtype* top_data = top[0]->gpu_data();
  Dtype* bottom_diff = bottom[0]->mutable_gpu_diff();
  Dtype* scale_data = scale_.mutable_gpu_data();
  int count = top[0]->count();
  int channels = top[0]->shape(multilogistic_axis_);
  caffe_copy(count, top_diff, bottom_diff);
  //scale : 1-top_data
  // NOLINT_NEXT_LINE(whitespace/operators)
  caffe_gpu_set(scale_.count(),Dtype(1),scale_data);
  kernel_sub<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, top_data, scale_data);
  kernel_mul<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, top_data, scale_data);
  kernel_mul<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, scale_data, bottom_diff);
}

INSTANTIATE_LAYER_GPU_FUNCS(MultilogisticLayer);


}  // namespace caffe
